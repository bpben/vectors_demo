{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb78f4-b61c-4e56-a1e2-153fb0482523",
   "metadata": {
    "id": "13eb78f4-b61c-4e56-a1e2-153fb0482523"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cfc03-000a-4381-a353-2778f97b2a36",
   "metadata": {
    "id": "219cfc03-000a-4381-a353-2778f97b2a36"
   },
   "source": [
    "## Representing text data\n",
    "This notebook is an introduction to using NLP tools to create informative representations of text data.  It accompanies [this slide deck]().  Refer to both for a complete understanding of this walkthrough.\n",
    "\n",
    "### Table of Contents\n",
    "* [Data processing](#data)\n",
    "* [Word Counts](#word)\n",
    "* [TF-IDF](#tfidf)\n",
    "* [Topic models](#topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7e591-7d7b-4d92-8f61-d5bb03af5dd7",
   "metadata": {
    "id": "77c7e591-7d7b-4d92-8f61-d5bb03af5dd7"
   },
   "source": [
    "### Data processing <a class=\"anchor\" id=\"data\"></a>\n",
    "\n",
    "Up first is some preprocessing.  You'll either need to download the [imdb review data](https://ai.stanford.edu/~amaas/data/sentiment/) and save it to this directory OR download the [processed data](https://drive.google.com/file/d/1oN_fO91IBkDHD_u6WXiUCvhhyNexQDJq/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c4ac0-5a90-49ef-b03b-378932d13c9a",
   "metadata": {
    "id": "9b3c4ac0-5a90-49ef-b03b-378932d13c9a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # processing the original data into DataFrame\n",
    "# # here for reference, don't need to run this if you're using reviews.pkl.gz\n",
    "# from pathlib import Path\n",
    "# source_path = Path('./aclImdb/')\n",
    "# #neg_files = source_path.glob('./*/neg/*.txt')\n",
    "# #pos_files = source_path.glob('./*/pos/*.txt')\n",
    "# all_files = []\n",
    "# for f in source_path.glob('./*/*/*.txt'):\n",
    "#     filename = f.as_posix()\n",
    "#     if 'unsup' not in filename:\n",
    "#         # split up into useful components\n",
    "#         _, split, sent, idx = filename.split('/')\n",
    "#         idx = int(idx.split('_')[0])\n",
    "#         all_files.append([idx, split, sent, f.read_text()])\n",
    "# review_df = pd.DataFrame(all_files)\n",
    "# review_df.columns = ['idx', 'split', 'label', 'text']\n",
    "# # some minor html cruft is in here\n",
    "# review_df['text'] = review_df['text'].str.replace('<br /><br />', '')\n",
    "# review_df = review_df.to_pickle('reviews.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f61ea-8e62-4d21-8f5a-6e3eb1bed2d3",
   "metadata": {
    "id": "e28f61ea-8e62-4d21-8f5a-6e3eb1bed2d3"
   },
   "outputs": [],
   "source": [
    "# can skip here if you already have reviews.pkl.gz\n",
    "review_df = pd.read_pickle('reviews.pkl.gz')\n",
    "# making this boolean for ease of use\n",
    "review_df['label'] = review_df['label'] == 'pos'\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db840c0f-9f6d-4058-9913-356eb700dcd0",
   "metadata": {
    "id": "db840c0f-9f6d-4058-9913-356eb700dcd0"
   },
   "source": [
    "### Word counts  <a class=\"anchor\" id=\"data\"></a>\n",
    "A very basic way to use a sanitized list of tokens is to do a word count. This unlocks a lot of insights right off and is an important step in exploratory data analysis in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2646a7d-b0f4-406d-bb68-7f5351b425b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2646a7d-b0f4-406d-bb68-7f5351b425b2",
    "outputId": "aecc0a46-0219-44fe-d77b-f4eb869838ca"
   },
   "outputs": [],
   "source": [
    "# take a positive and negative review for examples\n",
    "neg_review = review_df.loc[~review_df.label].iloc[0]['text']\n",
    "pos_review = review_df[review_df.label].iloc[0]['text']\n",
    "print('Negative\\n', neg_review, '\\n')\n",
    "print('Positive\\n', pos_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee60e2b-b1b5-4342-9e7e-8e0e3f17538f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ee60e2b-b1b5-4342-9e7e-8e0e3f17538f",
    "outputId": "08b1cb25-f98c-493f-ddba-a5bf608b6dc7"
   },
   "outputs": [],
   "source": [
    "# base python Counter - split on whitespace, use Counter object)\n",
    "counter_dtm = Counter(neg_review.split())\n",
    "print(counter_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c7f82-7e21-42a1-9f70-0c97345ab36a",
   "metadata": {
    "id": "112c7f82-7e21-42a1-9f70-0c97345ab36a"
   },
   "source": [
    "#### What do we notice already?\n",
    "\n",
    "* Capitalization matters!\n",
    "* Basic \"stopwords\" (the, it) dominate frequencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3996e",
   "metadata": {},
   "source": [
    "We'll be using scikit-learn's CountVectorizer, which makes some decisions for us by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3868f067-62e5-4227-9e81-76485ec7cfdd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3868f067-62e5-4227-9e81-76485ec7cfdd",
    "outputId": "3e507fce-cb89-4812-aae9-174ebb766e28"
   },
   "outputs": [],
   "source": [
    "# scikit-learn's countvectorizer\n",
    "count = CountVectorizer()\n",
    "neg_vec = count.fit_transform([neg_review])\n",
    "neg_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b990345-6215-4e55-845e-53ad8d5c5e80",
   "metadata": {
    "id": "0b990345-6215-4e55-845e-53ad8d5c5e80"
   },
   "source": [
    "`CountVectorizer` outputs a sparse matrix by default.  This is our \"document-term matrix\".  You can see since we have one document, the first dimension is 1.  76 corresponds to the 76 unique tokens that are part of the vectorizer's vocabulary.  \n",
    "\n",
    "We can create something compareable to the result from Counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628123c-2f0a-48b5-afa3-faf6284f60f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5628123c-2f0a-48b5-afa3-faf6284f60f5",
    "outputId": "5d9260a8-654e-4927-d319-950c9d533e89"
   },
   "outputs": [],
   "source": [
    "sk_dtm = dict(zip(count.get_feature_names_out(), \n",
    "                  neg_vec.toarray().flatten()))\n",
    "print(sk_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713c76d",
   "metadata": {},
   "source": [
    "Some things have changed! Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare two vocabularies\n",
    "set(counter_dtm.keys()).difference(set(sk_dtm.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9aa79",
   "metadata": {},
   "source": [
    "#### What changed?\n",
    "* Automatically lower-cased all tokens\n",
    "* Removed single-character tokens\n",
    "* Punctuation doesn't count!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f69c5-72b7-4ae9-8ed8-969818832c07",
   "metadata": {
    "id": "1e1f69c5-72b7-4ae9-8ed8-969818832c07"
   },
   "source": [
    "Let's set up a training and a test set with a 70/30 split.  Ideally you'd also have a validation set and use that for model selection, but I'm just keeping this simple for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to use this train/test split throughout\n",
    "# we'll also use this seed for consistency\n",
    "# NOTE: Usually you'll want to do a separate validation set when choosing models/featuresets!\n",
    "seed = 37\n",
    "np.random.seed(seed)\n",
    "pct_train = 0.7\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    review_df['text'],\n",
    "    review_df['label'], train_size=pct_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ed155",
   "metadata": {},
   "source": [
    "#### Sentiment analysis with Count Vectors\n",
    "Here we use the representation from out count vectorizer to try to predict sentiment.  \n",
    "\n",
    "Representation: Count vectors\n",
    "Application: Predict sentiment\n",
    "\n",
    "Here I use scikit-learn's [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).  This allows us to construct a complete pipeline that can be used to fit both vocabulary and model and then applied to the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9d696-97ba-415d-84d2-503844edf3b3",
   "metadata": {
    "id": "25a9d696-97ba-415d-84d2-503844edf3b3"
   },
   "outputs": [],
   "source": [
    "# setting up the vectorizer\n",
    "# limit the number of features - otherwise will have all the words in the corpus\n",
    "# removing stopwords (common words like 'the', 'and', etc.)\n",
    "count = CountVectorizer(stop_words='english', min_df=0.05)\n",
    "\n",
    "# pipeline contains the preprocessor and the model\n",
    "count_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", count),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952acf7e-5a15-4bb8-88d1-258a634e5cce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "952acf7e-5a15-4bb8-88d1-258a634e5cce",
    "outputId": "4ea3084b-d6e4-41e8-c067-94edcd54f667"
   },
   "outputs": [],
   "source": [
    "# setting the seed to keep the performance consistent\n",
    "np.random.seed(seed)\n",
    "count_pipeline.fit(X_train, y_train)\n",
    "# print out the size of the vocabulary\n",
    "print('Vocab size:', len(count_pipeline.named_steps['preprocessor'].vocabulary_))\n",
    "print('Accuracy on test set:', count_pipeline.score(X_test, y_test).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d9f7f-53b2-4cbb-92e6-2d5bac6dcf9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f58d9f7f-53b2-4cbb-92e6-2d5bac6dcf9a",
    "outputId": "45adbc5b-8498-45bd-b7be-5347af6bbbe4"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(y_pred=count_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f90bd",
   "metadata": {},
   "source": [
    "This is pretty good for a first pass.  81% of the time, we're predicting the correct sentiment.  But are there ways to do better?\n",
    "\n",
    "#### Experiments to improve our model\n",
    "Experiment with some of the parameters of [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to create different representations that may yield better results.\n",
    "\n",
    "* By including stopwords, we actually see minor improvements in performance\n",
    "* Expanding the number of features increases performance, but leads to overfitting\n",
    "* Experiments with n-grams can yield improvements\n",
    "\n",
    "#### What is driving performance?\n",
    "Are there particular words that are predictive in the featureset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2535aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize highest magnitude coefficients in model\n",
    "vocab = count_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "mag_ordered = np.argsort(np.abs(count_pipeline.named_steps['model'].coef_.flatten()))\n",
    "top = 10\n",
    "top_mag_ordered = mag_ordered[-10:]\n",
    "print('Top 10 coefficients by magnitude')\n",
    "for idx in top_mag_ordered:\n",
    "    print(vocab[idx], count_pipeline.named_steps['model'].coef_.flatten()[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30256a1-982a-4aee-877c-7a7045580aa2",
   "metadata": {
    "id": "a30256a1-982a-4aee-877c-7a7045580aa2"
   },
   "source": [
    "These coefficients are weights the model has learned based on the training data.  We may want to experiment with other methods for \"weighting\" the different features.  In this case we do it as part of the preprocessing.\n",
    "\n",
    "### TF-IDF <a class=\"anchor\" id=\"tfidf\"></a>\n",
    "One thing we notice with count vectors is that all words are being counted the same.  We might want to use a weighting scheme to ensure that words that are more informative about the content are flagged as more important.  One weighting scheme is Term Frequency - Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "Take as an example some kind of simplistic movie reviews.  We can already tell which words are most relevant to the specific content of each review (i.e. \"good\", \"bad\", \"great\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e0955-d159-4d27-8508-c1a5645a6dcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "385e0955-d159-4d27-8508-c1a5645a6dcd",
    "outputId": "d6a56558-184c-4d0d-e4c4-c02e771f41ad"
   },
   "outputs": [],
   "source": [
    "docs = ['The movie was good',\n",
    "        'The movie was bad',\n",
    "        'The movie was great']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "vecs = cv.fit_transform(docs).toarray()\n",
    "# we'll use pandas DF for easier display\n",
    "pd.DataFrame(vecs, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d93d3-aa56-4599-bd57-bd4e12232445",
   "metadata": {
    "id": "1f9d93d3-aa56-4599-bd57-bd4e12232445"
   },
   "source": [
    "You'll notice that `vecs` contains the term frequencies.  If we use sklearn's `TfidfVectorizer`, it will calculate those term counts and then multiply them by the Inverse Document Frequency (IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3611e-8860-411c-b297-e9d2eedca714",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "30f3611e-8860-411c-b297-e9d2eedca714",
    "outputId": "cc9c019b-8e82-47ed-e88b-3f215ed10068"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "# we'll use pandas DF for easier display\n",
    "tfidf_vecs = tfidf.fit_transform(docs).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_vecs, columns=tfidf.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464164b-5882-48d7-8bb9-f48008ca8d92",
   "metadata": {
    "id": "a464164b-5882-48d7-8bb9-f48008ca8d92"
   },
   "source": [
    "#### What do you notice here?\n",
    "What has happened due to the IDF weighting?\n",
    "* The words that make each review \"unique\" are up-weighted\n",
    "* Words that are in common are downweighted\n",
    "\n",
    "Note: Hand-calculation will get you different results - sklearn uses some adjustments for consistency and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f28bdf-371d-4dd7-a25b-63376f3f76d9",
   "metadata": {
    "id": "28f28bdf-371d-4dd7-a25b-63376f3f76d9"
   },
   "outputs": [],
   "source": [
    "# set up our tfidf vectorizer the same way as the count vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=0.05)\n",
    "\n",
    "tfidf_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", tfidf),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f2b41-18ed-4fab-adee-d5b7bd44c90a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c7f2b41-18ed-4fab-adee-d5b7bd44c90a",
    "outputId": "2576c265-49ff-43f3-b0ec-f51911ee83a5"
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "tfidf_pipeline.fit(X_train, y_train)\n",
    "print('Accuracy on test set:', tfidf_pipeline.score(X_test, y_test).round(2))\n",
    "print(\n",
    "    classification_report(y_pred=tfidf_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482b4e9-8a10-461a-93bb-cc1806ea8879",
   "metadata": {
    "id": "8482b4e9-8a10-461a-93bb-cc1806ea8879"
   },
   "source": [
    "The performance is pretty compareable here, it's useful to consider why that might be.\n",
    "\n",
    "#### Why might IDF-weighted representations not improve our performance?\n",
    "* Think what is the goal of IDF weighting\n",
    "* What is the goal of our sentiment analysis prediction?\n",
    "* What might be a better application of a representation like this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between positive and negative sentiment reviews\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# we'll use the tfidf vectorizer for this\n",
    "tfidf_vecs = tfidf.transform(X_test).toarray()\n",
    "cv_vecs = count.transform(X_test).toarray()\n",
    "# take the mean of similarity between positive and negative reviews\n",
    "print('Similarity between positive and negative review vectors')\n",
    "print('TFIDF:', cosine_similarity(tfidf_vecs[y_test], tfidf_vecs[~y_test]).mean(1).mean())\n",
    "print('Count:', cosine_similarity(cv_vecs[y_test], cv_vecs[~y_test]).mean(1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757fb91",
   "metadata": {},
   "source": [
    "In both examples above, we used many features.  It may be valuable to create a more \"generalized\" view of the words used in reviews.  This would give us some interpretability about the subjects covered and may yield a useful representation for other use cases.\n",
    "\n",
    "### Topic Models <a class=\"anchor\" id=\"topic\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5675d7-407d-4127-9858-42c0faaa7d34",
   "metadata": {
    "id": "1c5675d7-407d-4127-9858-42c0faaa7d34"
   },
   "outputs": [],
   "source": [
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component for topic models\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b7854-4689-4688-b8b2-bf6aca0de643",
   "metadata": {
    "id": "b68b7854-4689-4688-b8b2-bf6aca0de643"
   },
   "outputs": [],
   "source": [
    "# choose the number of components (topics)\n",
    "n_components = 10\n",
    "# adding a few tweaks, just based on experimentation\n",
    "nmf = NMF(n_components=n_components,\n",
    "          init='nndsvda',\n",
    "         max_iter=500)\n",
    "# NMF typically uses tfidf, not word counts\n",
    "# fit tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vecs = tfidf.fit_transform(review_df['text'])\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b799b42-2e02-4d29-a2d1-cae86544135d",
   "metadata": {
    "id": "0b799b42-2e02-4d29-a2d1-cae86544135d"
   },
   "source": [
    "Both NMF provides a components matrix which corresponds to the loading of each word on each topic.  Higher values means the word is more relevant to that topic.  With the function below, we can display some of the \"representative\" words from each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bcf3b-c3e9-4df8-b21c-e564c85addca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa3bcf3b-c3e9-4df8-b21c-e564c85addca",
    "outputId": "25372cf3-6509-46ef-8936-801309325839"
   },
   "outputs": [],
   "source": [
    "display_components(nmf, tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f9467",
   "metadata": {},
   "source": [
    "#### What do we observe about these topics?\n",
    "* Topic 7 - horror movies\n",
    "* Topic 6 - TV series reviews\n",
    "* Sample from these topics and assess your labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c0925-49b6-4baa-a666-d0b5c21ee2d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "573c0925-49b6-4baa-a666-d0b5c21ee2d5",
    "outputId": "167a8408-51e5-4381-e8e1-6d4cdec65991"
   },
   "outputs": [],
   "source": [
    "review_df.iloc[np.argsort(nmf_vecs[:, 7])[-2:]]['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b26740-a871-4c73-a42a-18a9835f5149",
   "metadata": {
    "id": "c0b26740-a871-4c73-a42a-18a9835f5149"
   },
   "source": [
    "Let's see how this does with sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963e47c-6508-45c7-8ff7-2c3742fee335",
   "metadata": {
    "id": "f963e47c-6508-45c7-8ff7-2c3742fee335"
   },
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=0.05)\n",
    "nmf = NMF(n_components=n_components,\n",
    "          init='nndsvda',\n",
    "          max_iter=500)\n",
    "clf_nmf_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", tfidf),\n",
    "           (\"topic\", nmf),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f3288-f8b7-4bd4-ae62-0bac6cb4bdf3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a02f3288-f8b7-4bd4-ae62-0bac6cb4bdf3",
    "outputId": "76f57b4e-87a9-409d-bbd7-3bde13f92b0c"
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "clf_nmf_pipeline.fit(X_train, y_train)\n",
    "print(f'accuracy: {clf_nmf_pipeline.score(X_test, y_test)}')\n",
    "print(\n",
    "    classification_report(y_pred=clf_nmf_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c03126-fd7a-48f3-ad36-ddd074fc4836",
   "metadata": {
    "id": "b4c03126-fd7a-48f3-ad36-ddd074fc4836"
   },
   "source": [
    "Again, slightly worse, but here we're looking at 10 features versus 255 in the other representations.  With some tuning, we might be able to get compareable performance with a much smaller feature vector.\n",
    "\n",
    "One thing this pipeline may be useful for is seeing performance across different \"topics\".\n",
    "\n",
    "#### Examine performance across topics\n",
    "* Identify the \"top\" topic for a particular review (`np.argmax`)\n",
    "* Use either the tfidf or count pipeline to predict sentiment\n",
    "* Examine the performance distribution and interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10173e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance of count pipeline by nmf component\n",
    "nmf_component = np.argmax(nmf.transform(tfidf.transform(X_test)), axis=1)\n",
    "tfidf_pred = tfidf_pipeline.predict(X_test)\n",
    "for i in range(n_components):\n",
    "    print(f'Component {i}')\n",
    "    print(f'Accuracy: {np.mean(tfidf_pred[nmf_component == i] == y_test[nmf_component == i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b91f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mainpy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
